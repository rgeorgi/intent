#!/usr/bin/env python
# encoding: utf-8
'''
wsj.txtify -- a WSJ-to-raw-text converter with.

wsj.txtify is a WSJ-to-raw-text converter

It defines a simple parser method.

@author:     Ryan Georgi
			
@copyright:  2013 Ryan Georgi. All rights reserved.
			
@license:    MIT License

@contact:    rgeorgi@uw.edu
@deffield    updated: Updated
'''

import sys
import os, glob
import ConfigParser
from utils.commandline import require_opt

from optparse import OptionParser
import re
from pos.TagMap import TagMap
from trees.ptb import parse_ptb_file
from treebanks.common import process_tree, write_files
from utils.systematizing import notify
from utils.ConfigFile import ConfigFile
from treebanks.TextParser import TextParser

__all__ = []
__version__ = 0.1
__date__ = '2013-08-26'
__updated__ = '2013-08-26'

DEBUG = 0
TESTRUN = 0
PROFILE = 0


class WSJParser(TextParser):
	
	def __init__(self, conf):
		self.conf = conf


	def parse(self):
		c = ConfigFile(self.conf)
	
		
		root = c['root']
		outdir = c['outdir']
		testfile = c['testfile']
		trainfile = c['trainfile']
		goldfile = c['goldfile']
		split = c.getint('trainsplit')
		maxlength = c.getint('maxlength')
		minlength = c['minlength']
		delimeter = c['delimeter']
		tagmap = c['tagmap']
		start_section = c['start_section']
		sentence_limit = c['sentence_limit']
		
		all_sents = []
		gold_sents = []
		
		tm = None
		if tagmap:
			tm = TagMap(pos_path=tagmap)
		
		posdir = os.path.join(root, 'tagged/wsj')
		rawdir = os.path.join(root, 'raw/wsj')
		

		# Filter the pos paths		
		pos_paths = map(lambda pos_path: os.path.join(posdir, pos_path), os.listdir(posdir))	
		pos_dirs = filter(lambda dir: os.path.isdir(dir), pos_paths)
		valid_pos_dirs = filter(lambda dir: int(os.path.basename(dir)) >= start_section, pos_dirs)
		
		# Filter the raw files
		raw_paths = map(lambda pos_path: os.path.join(rawdir, pos_path), os.listdir(rawdir))
		raw_dirs = filter(lambda dir: os.path.isdir(dir), raw_paths)
		valid_raw_dirs = filter(lambda dir: int(os.path.basename(dir)) >= start_section, raw_dirs)
		
		pos_files = []
		raw_files = []
		
		# Add the pos files
		for valid_pos_dir in valid_pos_dirs:
			for root, dir, files in os.walk(valid_pos_dir):
				
				for pos_path in filter(lambda x: x.startswith('wsj_'), files):
					pos_path = os.path.join(root, pos_path)
					pos_files.append(pos_path)
					
		# Add the raw files
		for valid_raw_dir in valid_raw_dirs:
			for root, dir, files in os.walk(valid_raw_dir):
				
				for pos_path in filter(lambda x: x.startswith('wsj_'), files):
					pos_path = os.path.join(root, pos_path)
					raw_files.append(pos_path)
				
				
		# "zip" the two lists together.
		assert len(pos_files) == len(raw_files)
		zipped_files = zip(pos_files, raw_files)
		
		
		
		# --) Number of sentences before bailing
		sentence_count = 0
		
		finished_processing = False
		
		# Keep track of the sentences being processed.
		all_sents = []
		gold_sents = []
		
		for pos_path, raw_path in zipped_files:
			
			raw_file = file(raw_path, 'r')
			raw_data = raw_file.read()
			raw_file.close()
			
			pos_file = file(pos_path, 'r')
			pos_data = pos_file.read()
			pos_file.close()
			
			# Now, process the raw data into sents.
			raw_data = re.sub('\.START\s+', '', raw_data)
			raw_sents = re.split('\n\n', raw_data.strip())
			
			# Now, get the tokens from the pos data.
			tokens = re.findall('(\S+)/(\S+)', pos_data)
						
			for sent in raw_sents:
				raw_str, pos_str = '', ''
				
				sys.stderr.write(sent+'\n')				
				while sent:
					word, tag = tokens.pop(0)
					print sent,
					sent = sent[len(word):].strip()
					print '-----', word
					raw_str += '%s ' % word
					pos_str += '%s%s%s ' % (word, delimeter, tag)

				
				all_sents.append(raw_str.strip())
				gold_sents.append(pos_str.strip())
				
				sentence_count += 1
				if sentence_count == sentence_limit:
					finished_processing = True
					break
			
			if finished_processing:
				break
					
		write_files(outdir, split, testfile, trainfile, goldfile, all_sents, gold_sents)
		notify()
						

def main(argv=None):
	'''Command line options.'''
	
	program_name = os.path.basename(sys.argv[0])
	program_version = "v0.1"
	program_build_date = "%s" % __updated__
	
	program_version_string = '%%prog %s (%s)' % (program_version, program_build_date)
	#program_usage = '''usage: spam two eggs''' # optional - will be autogenerated by optparse
	program_longdesc = '''''' # optional - give further explanation about what the program does
	program_license = "Copyright 2013 Ryan Georgi (Ryan Georgi)                                            \
				Licensed under the Apache License 2.0\nhttp://www.apache.org/licenses/LICENSE-2.0"
	
	if argv is None:
		argv = sys.argv[1:]

	# setup option parser
	parser = OptionParser(version=program_version_string, epilog=program_longdesc, description=program_license)
	parser.add_option("-c", "--conf", dest="conf", help="set conf file [default: %default]", metavar="FILE")
		
	# set defaults
	parser.set_defaults()
	
	# process options
	(opts, args) = parser.parse_args(argv)
	
	errors = require_opt(opts.conf, "Please specify the configuration file with -c or --conf", True)
		
	if errors:
		raise Exception("There were errors found in processing.")
	
	# MAIN BODY #	
	c = ConfigFile(opts.conf)
	p = WSJParser(opts.conf)
	p.parse()

	


		


if __name__ == "__main__":
	if DEBUG:
		sys.argv.append("-h")
	if TESTRUN:
		import doctest
		doctest.testmod()
	if PROFILE:
		import cProfile
		import pstats
		profile_filename = 'wsj.txtify_profile.txt'
		cProfile.run('main()', profile_filename)
		statsfile = open("profile_stats.txt", "wb")
		p = pstats.Stats(profile_filename, stream=statsfile)
		stats = p.strip_dirs().sort_stats('cumulative')
		stats.print_stats()
		statsfile.close()
		sys.exit(0)
	sys.exit(main())