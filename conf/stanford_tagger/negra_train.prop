## Sample properties file for maxent tagger. This file is used for three main
## operations: training, testing, and tagging. It may also be used to convert
## an old multifile tagger to a single file tagger or to dump the contents of
## a model.
## To train or test a model, or to tag something, run:
##   java edu.stanford.nlp.tagger.maxent.MaxentTagger -prop <properties file>
## Arguments can be overridden on the commandline, e.g.:
##   java ....MaxentTagger -prop <properties file> -testFile /other/file 

# Model file name (created at train time; used at tag and test time)
# (you can leave this blank and specify it on the commandline with -model)
model = negra.tagger

# Path to file to be operated on (trained from, tested against, or tagged)
# Specify -textFile <filename> to tag text in the given file, -trainFile <filename> to
# to train a model using data in the given file, or -testFile <filename> to test your
# model using data in the given file.  Alternatively, you may specify
# -dump <filename> to dump the parameters stored in a model or 
# -convertToSingleFile <filename> to save an old, multi-file model (specified as -model)
# to the new single file format.  The new model will be saved in the file filename.
# If you choose to convert an old file, you must specify 
# the correct 'arch' parameter used to create the original model.
# trainFile = 

# Path to outputFile to write tagged output to.
# If empty, stdout is used.
outputFile = /Users/rgeorgi/Dropbox/code/eclipse/prototype-sequence/data/negra_tagged.txt 

# Output format. One of: slashTags (default), xml, or tsv
# outputFormat = slashTags

# Output format options. Comma separated list, but
# currently "lemmatize" is the only supported option.
# outputFormatOptions = 

# Tag separator character that separates word and pos tags
# (for both training and test data) and used for
# separating words and tags in slashTags format output.
# tagSeparator = /

# Encoding format in which files are stored.  If left blank, UTF-8 is assumed.
# encoding = UTF-8

# A couple flags for controlling the amount of output:
# - print extra debugging information:
# verbose = false
# - print intermediate results:
# verboseResults = true
######### parameters for tag and test operations #########

# Class to use for tokenization. Default blank value means Penn Treebank
# tokenization.  If you'd like to just assume that tokenization has been done,
# and the input is whitespace-tokenized, use
# edu.stanford.nlp.process.WhitespaceTokenizer or set tokenize to false.
# tokenizerFactory = 

# Options to the tokenizer.  A comma separated list.
# This depends on what the tokenizer supports.
# For PTBTokenizer, you might try options like americanize=false
# or asciiQuotes (for German!).
# tokenizerOptions = 

# Whether to tokenize text for tag and test operations. Default is true.
# If false, your text must already be whitespace tokenized.
# tokenize = true

# Write debugging information (words, top words, unknown words). Useful for
# error analysis. Default is false.
# debug = false

# Prefix for debugging output (if debug == true). Default is to use the
# filename from 'file'
# debugPrefix = 

######### parameters for training  #########

# model architecture: This is one or more comma separated strings, which
# specify which extractors to use. Some of them take one or more integer
# or string 
# (file path) arguments in parentheses, written as m, n, and s below:
# 'left3words', 'left5words', 'bidirectional', 'bidirectional5words',
# 'generic', 'sighan2005', 'german', 'words(m,n)', 'wordshapes(m,n)',
# 'biwords(m,n)', 'lowercasewords(m,n)', 'vbn(n)', distsimconjunction(s,m,n)',
# 'naacl2003unknowns', 'naacl2003conjunctions', 'distsim(s,m,n)',
# 'suffix(n)', 'prefix(n)', 'prefixsuffix(n)', 'capitalizationsuffix(n)',
# 'wordshapes(m,n)', 'unicodeshapes(m,n)', 'unicodeshapeconjunction(m,n)',
# 'lctagfeatures', 'order(k)', 'chinesedictionaryfeatures(s)'.
# These keywords determines the features extracted.  'generic' is language independent.
# distsim: Distributional similarity classes can be an added source of information
# about your words. An English distsim file is included, or you can use your own.
# arch = generic 

# 'wordFunction'.  A function applied to the text before training or tagging.
# For example, edu.stanford.nlp.util.LowercaseFunction
# This function turns all the words into lowercase
# The function must implement edu.stanford.nlp.util.Function<String, String>
# Blank means no preprocessing function
# wordFunction = 

# 'language'.  This is really the tag set which is used for the
# list of open-class tags, and perhaps deterministic  tag
# expansion). Currently we have 'english', 'arabic', 'german', 'chinese'
# or 'polish' predefined. For your own language, you can specify 
# the same information via openClassTags or closedClassTags below
# (only ONE of these three options may be specified). 
# 'english' means UPenn English treebank tags. 'german' is STTS
# 'chinese' is CTB, and Arabic is an expanded Bies mapping from the ATB
# 'polish' means some tags that some guy on the internet once used. 
# See the TTags class for more information.
lang = german 

# a space-delimited list of open-class parts of speech
# alternatively, you can specify language above to use a pre-defined list or specify the closed class tags (below)
# openClassTags = 

# a space-delimited list of closed-class parts of speech
# alternatively, you can specify language above to use a pre-defined list or specify the open class tags (above)
# closedClassTags = 

# A boolean indicating whether you would like the trained model to set POS tags as closed
# based on their frequency in training; default is false.  The frequency threshold can be set below. 
# This option is ignored if any of {openClassTags, closedClassTags, lang} are specified.
# learnClosedClassTags = 

# Used only if learnClosedClassTags=true.  Tags that have fewer tokens than this threshold are
# considered closed in the trained model.
# closedClassTagThreshold = 

# search method for optimization. Normally use the default 'qn'. choices: 'qn' (quasi-Newton),
# 'cg' (conjugate gradient, 'owlqn' (L1 regularization) or 'iis' (improved iterative scaling)
# search = qn

# for conjugate gradient or quasi-Newton search, sigma-squared smoothing/regularization
# parameter. if left blank, the default is 0.5, which is usually okay
# sigmaSquared = 0.5

# for OWLQN search, regularization
# parameter. if left blank, the default is 1.0, which is usually okay
# regL1 = 1.0

# For improved iterative scaling, the number of iterations, otherwise ignored
# iterations = 100

# rare word threshold. words that occur less than this number of
# times are considered rare words.
# rareWordThresh = 5

# minimum feature threshold. features whose history appears less
# than this number of times are ignored.
# minFeatureThresh = 5

# current word feature threshold. words that occur more than this
# number of times will generate features with all of their occurring
# tags.
# curWordMinFeatureThresh = 2

# rare word minimum feature threshold. features of rare words whose histories
# appear less than this times will be ignored.
# rareWordMinFeatureThresh = 10

# very common word threshold. words that occur more than this number of
# times will form an equivalence class by themselves. ignored unless
# you are using equivalence classes.
# veryCommonWordThresh = 250

# sgml = 
# tagInside = 

# testFile and textFile can use multiple threads to process text.
# nthreads = 1
